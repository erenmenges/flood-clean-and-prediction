This is about NYC flood sensors dataset that I have access to. I want to use ML/DL to help the city. First, I will use ML to clean the dataset of complex noise. Then, I will train a hyperlocal flood prediction ML model prototype. My end goal is to develop a citywide flood warning system. However, currently, I will only work on this up to the model prototype.
The project consists of two modules as of now:
Module 1: Clean complex noise accurately using ML
Module 2: Train an ML model on it to predict floods.
I can invest up to 15 hours of data labeling, if it's going to help. Then I will have 30 hours to finish the project up until the end of the first flood prediction model prototype. I've already decided to:
1) Have the denoising model to create per-minute labelings for noise. However, I will do the manual labeling in chunks. The model will see that every minute in that chunk has that label.
2) I will train a classical + deep model for both modules. Classical baseline for denoising, then a deep one. Classical baseline for prediction, then a deep one. Compute is not a constraint at all.
3) The denoise model will not reconstruct real depth, it will mark every timestep as real or noise. Then it will mask them.
4) The prediction model will also be trained on / will have access to external data such as weather, elevation and tide.
5) The prediction model will also have sensor id as a feature. I don't want to train a different model per sensor. The model should be generalizable.
6) The prediction model will do regression, not classification. I will feed it 60 minutes of sensor data and ask it to predict the depth for the next 30 minutes.

Model choices:
1) classical ML for denoising: XGBoost
2) deep learning for denoising: Bi-directional LSTM
3) classical ML for prediction: XGBoost regressor
4) deep ML for prediction: Seq2Seq LSTM Encoder-Decoder with Attention


I'm not planning to use depth_raw_mm or depth_filt_mm at all.  we already have proc_mm, which has excellent heuristics applied to it. they are very conservative. you can check yourself in the document. The documentation admits that "complex noise" (chaotic chains, weird reflections) does slip through because the heuristics are so conservative. they say that their heuristic for removing flash floods looks for the rate that is 6x of the biggest they've measured, which was in a hurricane. any model that will denoise won't denoise raw_mm, it will denoise proc_mm. I will work with depth_proc_mm throughout the whole project.

A “flood minute” is: depth_proc_mm > 0 and not complex noise.

A “noise minute” is: depth_proc_mm > 0 but caused by blips/boxes/complex noise that slipped through the heuristics, not real water.

Minutes with depth_proc_mm == 0 are just “no water seen” and don’t need denoising.



So Module 1 is essentially:
For each minute where depth_proc_mm > 0, classify it as valid water vs noise, then mask the noise.


I know there is a flaw in module 2: The "Zero Problem." 99% of my target values are 0. A model that predicts "0" for everything achieves 99% accuracy but fails 100% of the time at flood warning. I will counter this by using a weighted loss function.



roadmap:
1) smart sampling for labeling. I will sample events. an event is defined as a contiguous run of minutes where depth_proc_mm > 0, padded with 30 minutes of context before and after. Gaps of up to 15 minutes of 0's or NaN's will be tolerated and merged. This treats a flood with a few missing pings (common due to sensor issues) as one cohesive event. I will label events as "0 - noise", and "1 - real".

I will have three sampling buckets:

First bucket: The phantoms. This will consist 40% of the manual labeling set. It will have depth_proc_mm > 0, cumulative precipitation in the last 120 minutes = 0, and Precipitation (Last 24 Hours) < (some value).

Second bucket: The Gray Zone. This will consist 40% of the set. It will have depth_proc_mm > 0, but 0 < cumulative precipitation in the last 120 minutes < (some low value) or tide > (some medium value). It might be a puddle, or a glitch!

Third bucket: The real floods. This will consist 20% of the set. It will have depth_proc_mm > (some high value), cumulative precipitation in the last 120 minutes > (some high value) or tide > (some high value).

2) The denoising model

Model architecture:
Total Window: 64 minutes.
Past Context: 59 minutes.
Target: The 60th minute.
Future Context: 5 minutes.

This means the real-world system will also have a lag of 5 minutes, but that's okay.